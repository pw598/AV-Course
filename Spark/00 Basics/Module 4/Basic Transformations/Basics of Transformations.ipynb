{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXqHMVdqIi22"
   },
   "source": [
    "# Basics of Transformations Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBjkE9YGIi29"
   },
   "source": [
    "In Spark Streaming, DStreams are treated very similarly to the RDDs that make them up. Like RDDs, there are a wide variety of data transformation options. \n",
    "\n",
    "Here are some examples of the transformations from the Spark documentation that might be useful for your purposes\n",
    "\n",
    "| Transformation        | Meaning         |\n",
    "| ------------------------------ |:-------------|\n",
    "| **map**(func)      | Return a new DStream by passing each element of the source DStream through a function func.    |\n",
    "| **flatMap**(func)\t| Similar to map, but each input item can be mapped to 0 or more output items.    |\n",
    "| **filter**(func)\t| Return a new DStream by selecting only the records of the source DStream on which func returns true.    |\n",
    "| **repartition**(numPartitions)\t| Changes the level of parallelism in this DStream by creating more or fewer partitions.    |\n",
    "| **union**(otherStream)\t| Return a new DStream that contains the union of the elements in the source DStream and otherDStream. |\n",
    "| **count**()\t| Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.  |\n",
    "| **reduce**(func)\t| Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using  a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n",
    "| **countByValue**()\t| When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n",
    "| **reduceByKey**(func, [numTasks])\t| When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "| **join**(otherStream, [numTasks])\t| When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "| **cogroup**(otherStream, [numTasks])\t| When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n",
    "\n",
    "\n",
    "If you look at the spark streaming documentation, you will also find the `transform(func)` and `updateStateByKey(func)`. We will discuss these later in the course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFF7OzwyIi3A"
   },
   "source": [
    "### Demo (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "HGUndzNAIi3B"
   },
   "source": [
    "We're going to be demoing the map and flatmap functions with respect to DStreams. One important question is \"What is the difference between the two?\"\n",
    "\n",
    "`map`: It returns a new RDD by applying a function to each element of the RDD. Function in map can return only one item. Works with DStreams as well as RDDs\n",
    "\n",
    "`flatMap`: Similar to map, it returns a new RDD by applying  a function to each element of the RDD, but output is flattened.\n",
    "Also, function in flatMap can return a list of elements (0 or more). Works with DStreams as well as RDDs.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIPY1y94mMjC"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8ybQ2KznV7Y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1iFB6s7nWPm"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvqR8fscIi3B"
   },
   "outputs": [],
   "source": [
    "# The first step is to import the required libraries \n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6895,
     "status": "ok",
     "timestamp": 1637584243272,
     "user": {
      "displayName": "Siddharth Sonkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgyC4_J9Jm0jUPwMM_zelgFJvfG6c3il_J1gs9A=s64",
      "userId": "17600273976509340005"
     },
     "user_tz": -330
    },
    "id": "vw5y9MQFIi3E",
    "outputId": "d2e015ed-21a1-4b3c-f67c-e013e9a9827f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(1, 3), range(1, 4), range(1, 5)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a SparkContext \n",
    "\n",
    "sc = SparkContext(appName=\"PythonStreamingTransformationDemo\")\n",
    "\n",
    "# Usage of map() function. Parallelize and collect sample data\n",
    "sc.parallelize([3,4,5]).map(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1637584243273,
     "user": {
      "displayName": "Siddharth Sonkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgyC4_J9Jm0jUPwMM_zelgFJvfG6c3il_J1gs9A=s64",
      "userId": "17600273976509340005"
     },
     "user_tz": -330
    },
    "id": "hjYuixFWIi3E",
    "outputId": "840d391f-1c0a-46a4-b84c-6d7f9fefbc07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 3, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage of flatMap() function. Parallelize and collect sample data\n",
    "sc.parallelize([3,4,5]).flatMap(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TODVkKo8Ii3F"
   },
   "source": [
    "notice o/p is flattened out in a single list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UksQYDLJIi3G"
   },
   "source": [
    "Here's Another Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1637584243273,
     "user": {
      "displayName": "Siddharth Sonkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgyC4_J9Jm0jUPwMM_zelgFJvfG6c3il_J1gs9A=s64",
      "userId": "17600273976509340005"
     },
     "user_tz": -330
    },
    "id": "sauEp4SZIi3H",
    "outputId": "02ddcfcb-f679-46d7-d202-79c628f50ade"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9], [4, 16], [5, 25]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1637584243998,
     "user": {
      "displayName": "Siddharth Sonkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgyC4_J9Jm0jUPwMM_zelgFJvfG6c3il_J1gs9A=s64",
      "userId": "17600273976509340005"
     },
     "user_tz": -330
    },
    "id": "xdcUA42iIi3H",
    "outputId": "b2e05c45-dca8-43b5-ee66-a82825e4d822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbjRrKM0Ii3H"
   },
   "source": [
    "notice that the list is flattened in the latter version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQusX6P5Ii3I"
   },
   "source": [
    "Here's another example, this time interacting with a file, which can often be useful for debugging code that interacts with full DStreams\n",
    "\n",
    "There is a text file `greetings.txt` with following lines:\n",
    "```\n",
    "Good Morning\n",
    "Good Evening\n",
    "Good Day\n",
    "Happy Birthday\n",
    "Happy New Year\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEbkKcxFolN4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# loading Customer transactional data\n",
    "cust_trans=pd.read_csv(\"/content/drive/My Drive/Amex Hackathon/customer_transaction_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "error",
     "timestamp": 1637584244428,
     "user": {
      "displayName": "Siddharth Sonkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgyC4_J9Jm0jUPwMM_zelgFJvfG6c3il_J1gs9A=s64",
      "userId": "17600273976509340005"
     },
     "user_tz": -330
    },
    "id": "s7-2XksxIi3I",
    "outputId": "7ea6562b-ec86-4227-ebc4-942b1b1def85"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a1ea0cb1131e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"greetings.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/content/spark-3.2.0-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \"\"\"\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.2.0-bin-hadoop3.2/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.2.0-bin-hadoop3.2/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/greetings.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Input path does not exist: file:/content/greetings.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 34 more\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"greetings.txt\")\n",
    "lines.map(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHX895GCIi3J"
   },
   "outputs": [],
   "source": [
    "lines.flatMap(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSswm_IaIi3J"
   },
   "source": [
    "# Demo (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ2xZVZ0Ii3J"
   },
   "source": [
    "Last time we went over the `map` and `flapmap` functions. We'll explore a few other options.\n",
    "\n",
    "Suppose we have a this example text from Dr Suess's _The Cat in the Hat_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf9aek_vIi3J"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djsq7BvIIi3K"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master=\"local[2]\", appName=\"DrSeussExample\")\n",
    "scc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoFh-ZLCIi3K"
   },
   "outputs": [],
   "source": [
    "myFile = scc.sparkContext.textFile(\"/DrSeuss.text\")\n",
    "wordspair = myFile.flatMap(lambda row: row.split(\" \")).map(lambda x: (x, 1)).reduceByKey(lambda x,y : x + y)\n",
    "oldwordcount = wordspair.reduceByKey(lambda x,y : x + y)\n",
    "lines = scc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUCk3BT_Ii3K"
   },
   "outputs": [],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DbT1x4uIi3L"
   },
   "source": [
    "Suppose then that we want to get wordcounts for this. We can use the map function from before here. `map` returns a new RDD containing values created by applying the supplied function to each value in the original RDD Here we use a lambda function which replaces some common punctuation characters with spaces and convert to lower  case, producing a new RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwnROfI2Ii3M"
   },
   "outputs": [],
   "source": [
    "wordcounts1 = lines.map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\n",
    "wordcounts1top = wordcounts1.transform(lambda rdd: rdd.take(10))\n",
    "wordcounts1top.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8KzRpNjIi3M"
   },
   "source": [
    "The flatMap function takes these input values and returns a new, flattened list. In this case, the lines are split into words and then each word becomes a separate value in the output RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcsPYE1fIi3M"
   },
   "outputs": [],
   "source": [
    "wordcounts2 = wordcounts1.flatMap(lambda x: x.split())\n",
    "wordcounts2top = wordcounts2.transform(lambda rdd: rdd.take(10))\n",
    "wordcounts2.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvYT26sZIi3N"
   },
   "source": [
    "Expect that the input RDD contains tuples of the form (key,value). Create a new RDD containing a tuple for each unique value of key in the input, where the value in the second position of the tuple is created by  applying the supplied lambda function to the values with the matching key in the input RDD Here the key will be the word and lambda function will sum up the word counts for each word. The output RDD  will consist of a single tuple for each unique word in the data, where the word is stored at the first position  in the tuple and the word count is stored at the second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYTYFun3Ii3N"
   },
   "outputs": [],
   "source": [
    "wordcounts3 = wordcounts2.map(lambda x: (x, 1))\n",
    "wordcounts3top = wordcounts3.transform(lambda rdd: rdd.take(20))\n",
    "wordcounts3.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55V6PwsPIi3N"
   },
   "outputs": [],
   "source": [
    "wordcounts4 = wordcounts3.reduceByKey(lambda x,y:x+y)\n",
    "wordcounts4top = wordcounts4.transform(lambda rdd: rdd.take(20))\n",
    "wordcounts4.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwZHmU6nIi3N"
   },
   "source": [
    "map a lambda function to the data which will swap over the first and second values in each tuple, now the word count appears in the first position and the word in the second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qsfVdT7Ii3N"
   },
   "outputs": [],
   "source": [
    "wordcounts5 = wordcounts4.map(lambda x:(x[1],x[0]))\n",
    "wordcounts5top = wordcounts5.transform(lambda rdd: rdd.take(20))\n",
    "wordcounts5.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgvt1dE2Ii3O"
   },
   "source": [
    "we sort the input RDD by the key value (i.e., the value at the first position in each tuple). In this example the first position stores the word count so this will sort the words so that the most frequently occurring words occur first in the RDD. The ascending=False parameter results in a descending sort order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKI-BEfwIi3O"
   },
   "outputs": [],
   "source": [
    "wordcounts6 = wordcounts5.sortByKey(ascending=False)\n",
    "wordcounts6top = wordcounts6.transform(lambda rdd: rdd.take(20))\n",
    "wordcounts6.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeYD0ZerIi3O"
   },
   "source": [
    "# References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fw6dBjq4Ii3O"
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Basics of Transformations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
