{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens - 100K Dataset\n",
    "\n",
    "MovieLens 100K dataset has been a standard dataset used for benchmarking recommender systems for more than 20 years now and hence this provides a good point to start our learning journey for recommender systems. For non commercial personalised recommendations for movies you can check out the website: https://movielens.org/\n",
    "\n",
    "This data set consists of:\n",
    "\t* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "\t* Each user has rated at least 20 movies. \n",
    "        * Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n",
    "The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. \n",
    "\n",
    "## Data Description\n",
    "\n",
    "\n",
    "**Ratings**    -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
    "              Each user has rated at least 20 movies.  Users and items are\n",
    "              numbered consecutively from 1.  The data is randomly\n",
    "              ordered. This is a comma separated list of \n",
    "\t         user id | item id | rating | timestamp. \n",
    "              The time stamps are unix seconds since 1/1/1970 UTC   \n",
    "\n",
    "\n",
    "**Movie Information**   -- Information about the items (movies); this is a comma separated\n",
    "              list of\n",
    "              movie id | movie title | release date | unknown | Action | Adventure | Animation |\n",
    "              Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "              Thriller | War | Western |\n",
    "              The last 19 fields are the genres, a 1 indicates the movie\n",
    "              is of that genre, a 0 indicates it is not; movies can be in\n",
    "              several genres at once.\n",
    "\n",
    "\n",
    "**User Demographics**    -- Demographic information about the users; this is a comma\n",
    "              separated list of\n",
    "              user id | age | gender | occupation | zip code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "[1. Reading Dataset](#Reading-Dataset)\n",
    "\n",
    "[2. Merging Movie information to ratings dataframe](#merge)\n",
    "\n",
    "[3. Creating Binary Relevance Target from User Average](#br)\n",
    "\n",
    "[4. Fitting SVD to make rating predictions](#svdfit)\n",
    "\n",
    "[5. Root Mean Squared Error](#rmse)\n",
    "\n",
    "[6. Creating Predicted List of relevant movies](#pbr)\n",
    "\n",
    "[7. Precision@K](#patk)\n",
    "\n",
    "[8. Recall@K](#ratk)\n",
    "\n",
    "[9. Mean Reciprocal Rank](#mrr)\n",
    "\n",
    "[10. Mean Average Precision@K](#mapk)\n",
    "\n",
    "[11. Normalised Discounted Cumulative Gain@K](#mapk)\n",
    "\n",
    "[12. What's next?](#whatsnext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Dataset <a class=\"anchor\" id=\"Reading-Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading ratings file:\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "\n",
    "#Reading Movie Info File\n",
    "movie_info = pd.read_csv('movie_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Merging Movie information to ratings dataframe <a class=\"anchor\" id=\"merge\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie names are contained in a separate file. Let's merge that data with ratings and store it in ratings dataframe. The idea is to bring movie title information in ratings dataframe as it would be useful later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.merge(movie_info[['movie id','movie title']], how='left', left_on = 'movie_id', right_on = 'movie id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "      <th>movie id</th>\n",
       "      <th>movie title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "      <td>242</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "      <td>302</td>\n",
       "      <td>L.A. Confidential (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "      <td>377</td>\n",
       "      <td>Heavyweights (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "      <td>51</td>\n",
       "      <td>Legends of the Fall (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "      <td>346</td>\n",
       "      <td>Jackie Brown (1997)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  unix_timestamp  movie id  \\\n",
       "0      196       242       3       881250949       242   \n",
       "1      186       302       3       891717742       302   \n",
       "2       22       377       1       878887116       377   \n",
       "3      244        51       2       880606923        51   \n",
       "4      166       346       1       886397596       346   \n",
       "\n",
       "                  movie title  \n",
       "0                Kolya (1996)  \n",
       "1    L.A. Confidential (1997)  \n",
       "2         Heavyweights (1994)  \n",
       "3  Legends of the Fall (1994)  \n",
       "4         Jackie Brown (1997)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also combine movie id and movie title separated by ': ' and store it in a new column named movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['movie'] = ratings['movie_id'].map(str) + str(': ') + ratings['movie title'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'movie_id', 'rating', 'unix_timestamp', 'movie id',\n",
       "       'movie title', 'movie'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the columns movie, user_id and rating in the ratings dataframe and drop all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop(['movie id', 'movie title', 'movie_id','unix_timestamp'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[['user_id','movie','rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Binary Relevance Target from User Average <a class=\"anchor\" id=\"br\"></a>\n",
    "For checking and using the classification accuracy metrics, we cannot work with ratings, so we need to create a new target from rating values which is 1 if the user has rated the movie greater than its user average and 0 if rated less than the user average. \n",
    "\n",
    "Post that we will create a list of relevant movies for all users with atleast one relevant movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Train test split in order to extract average user rating from train set\n",
    "X_train, X_test = train_test_split(ratings, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rating for each user using the ratings in train set\n",
    "ratings_new = ratings.merge(X_train.groupby('user_id')['rating'].mean(), left_on = 'user_id', right_index=True, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming columns for better readability\n",
    "ratings_new.columns = ['user_id', 'movie', 'actual_rating', 'user_average', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie</th>\n",
       "      <th>actual_rating</th>\n",
       "      <th>user_average</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242: Kolya (1996)</td>\n",
       "      <td>3</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302: L.A. Confidential (1997)</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377: Heavyweights (1994)</td>\n",
       "      <td>1</td>\n",
       "      <td>3.288660</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51: Legends of the Fall (1994)</td>\n",
       "      <td>2</td>\n",
       "      <td>3.628415</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346: Jackie Brown (1997)</td>\n",
       "      <td>1</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                           movie  actual_rating  user_average  \\\n",
       "0      196               242: Kolya (1996)              3      3.571429   \n",
       "1      186   302: L.A. Confidential (1997)              3      3.333333   \n",
       "2       22        377: Heavyweights (1994)              1      3.288660   \n",
       "3      244  51: Legends of the Fall (1994)              2      3.628415   \n",
       "4      166        346: Jackie Brown (1997)              1      3.500000   \n",
       "\n",
       "   relevant  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relevance for each user movie combination\n",
    "ratings_new['relevant'] = (ratings_new['actual_rating'] >= ratings_new['user_average']).astype(int)\n",
    "ratings_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.54337\n",
       "0    0.45663\n",
       "Name: relevant, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Relevance Distribution\n",
    "ratings_new['relevant'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and test datasets for the new ratings dataframe\n",
    "X_train, X_test = train_test_split(ratings_new, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating ground truth list of top n movies based on test data ratings for all users with atleast 1 relevant movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by the actual rating in the test set for each user in descending order\n",
    "X_test = X_test.sort_values(by = ['user_id','actual_rating'], ascending = [True, False])\n",
    "\n",
    "# Creating a list of relevant movies for each user\n",
    "X_test_list = X_test[X_test['relevant'] == 1].groupby(['user_id'])['movie'].apply(lambda x: x.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "1    [100: Fargo (1996), 181: Return of the Jedi (1...\n",
       "2    [242: Kolya (1996), 302: L.A. Confidential (19...\n",
       "3    [328: Conspiracy Theory (1997), 181: Return of...\n",
       "4                               [50: Star Wars (1977)]\n",
       "5    [428: Harold and Maude (1971), 390: Fear of a ...\n",
       "Name: movie, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Actual Relevant movies for all users with atleast 1 relevant movie\n",
    "X_test_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting SVD to make rating predictions <a class=\"anchor\" id=\"svdfit\"></a>\n",
    "Now we will fit the best SVD model with 11 factors and predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fd304a78a50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing functions to be used in this notebook from Surprise Package\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "#Reader object to import ratings from X_train\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "#Storing Data in surprise format from X_train\n",
    "data = Dataset.load_from_df(X_train[['user_id','movie','actual_rating']], reader)\n",
    "\n",
    "#Fitting the model on train data with the best parameters\n",
    "model = SVD(n_factors = 11, n_epochs = 20, random_state = 42)\n",
    "\n",
    "#Build full trainset will essentially fits the SVD on the complete train set instead of a part of it\n",
    "#like we do in cross validation for grid search\n",
    "model.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Root Mean Squared Error (RMSE) <a class=\"anchor\" id=\"rmse\"></a>\n",
    "First let us check the RMSE for the model which is a simple predictive accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that computes the root mean squared error (or RMSE)\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9390125163978545"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id pairs for test set\n",
    "id_pairs = zip(X_test['user_id'], X_test['movie'])\n",
    "\n",
    "#Making predictions for test set using predict method from Surprise\n",
    "y_pred = [model.predict(uid = user, iid = movie)[3] for (user, movie) in id_pairs]\n",
    "\n",
    "#Actual rating values for test set\n",
    "y_true = X_test['actual_rating']\n",
    "\n",
    "# Checking performance on test set\n",
    "rmse(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Predicted List of relevant movies <a class=\"anchor\" id=\"pbr\"></a>\n",
    "Now it is time to create a ranked list of top movies based on predicted ratings to demonstrate the use of accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a copy of test set\n",
    "X_test_pred = X_test.copy()\n",
    "\n",
    "#Predicted Rating for each user\n",
    "X_test_pred['pred_rating'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted relevance based on average user rating\n",
    "X_test_pred['pred_relevance'] = (X_test_pred['pred_rating'] >= X_test_pred['user_average']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating predicted ranked lists of movies for all users with atleast 1 predicted relevant movie\n",
    "X_test_pred = X_test_pred.sort_values(by = ['user_id','pred_rating'], ascending = [True, False])\n",
    "X_test_pred_list = X_test_pred[X_test_pred['pred_relevance'] == 1].groupby(['user_id'])['movie'].apply(lambda x: x.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keeping the common users in both lists\n",
    "X_test_pred_list = X_test_pred_list[X_test_pred_list.index.isin(X_test_list.index)].sort_index()\n",
    "X_test_list = X_test_list[X_test_list.index.isin(X_test_pred_list.index)].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Precision @ K <a class=\"anchor\" id=\"patk\"></a>\n",
    "Let's calculate Precision@10 for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Precision using the formula\n",
    "def precision(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5395652173913044"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patk_list = []\n",
    "for i,j in zip(X_test_list, X_test_pred_list):\n",
    "    pr = precision(i,j,10)\n",
    "    patk_list.append(pr)\n",
    "    \n",
    "np.mean(patk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recall @ K <a class=\"anchor\" id=\"ratk\"></a>\n",
    "Let's calculate Recall@10 for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Recall using the formula\n",
    "def recall(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5246095601689543"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_list = []\n",
    "for i,j in zip(X_test_list, X_test_pred_list):\n",
    "    re = recall(i,j,10)\n",
    "    recall_list.append(re)\n",
    "    \n",
    "np.mean(recall_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Mean Reciprocal Rank <a class=\"anchor\" id=\"mrr\"></a>\n",
    "Implementing mean reciprocal rank with binary relevance created list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017333953433209728\n"
     ]
    }
   ],
   "source": [
    "#Calculating Mean Reciprocal Rank\n",
    "rr_list = []\n",
    "users = X_test['user_id'].unique()\n",
    "mrm = pd.Series(X_test_list.index).apply(lambda x: X_test_list.loc[x][0]).tolist()\n",
    "frm = pd.DataFrame({'user_id':X_test_list.index, 'frm':mrm})\n",
    "\n",
    "for count,movie in enumerate(mrm):\n",
    "    try:\n",
    "        rr = 1/(1 + X_test_pred_list[count+1].index(movie))\n",
    "    except:\n",
    "        rr = 0\n",
    "    rr_list.append(rr)\n",
    "mrr = np.mean(rr_list)\n",
    "print(mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Mean Average Precision @ K <a class=\"anchor\" id=\"mapk\"></a>\n",
    "Here we will calculate Mean Average Precision @10 and check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of MAP@K\n",
    "def apk(actual, predicted, k=10):\n",
    "    #Clipping the predicted value till k\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    \n",
    "    #Calculating Precision\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    #Average Precision @K\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "#Calculating average across all users\n",
    "def mapk(actual, predicted, k=10):\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6675380434782608"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating MAP@5 for our use case\n",
    "mapk(X_test_list, X_test_pred_list, k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Normalised Discounted Cumulative Gain@K <a class=\"anchor\" id=\"ndcg\"></a>\n",
    "This time we will not user binray relevance but create a predicted list using predicted ratings and an use the actual ratings to implement NDCG as per discussion in the video for Rank Aware Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ranking with predicted rating\n",
    "dff = X_test_pred.sort_values(by = ['user_id','pred_rating'], ascending = [True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding actual ratings corresponding to the ranked recommended list\n",
    "dff_temp = dff.groupby(['user_id'])['actual_rating'].apply(lambda x: x.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating DCG\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "\n",
    "#Dividing DCG by IDCG (DCG Max) to get NDCG@K\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    \n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050174244115038"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating NDCG@10 for given use case\n",
    "ndcg_list = []\n",
    "for i in dff_temp.index:\n",
    "    ndcg_user = ndcg_at_k(r = dff_temp[i], k=10)\n",
    "    ndcg_list.append(ndcg_user)\n",
    "np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. What's Next? <a class=\"anchor\" id=\"whatsnext\"></a>\n",
    "Now you can try different algorithms and opimize for metrics by using these functions for any other problem as well. The Rank Aware metrics are especially useful for content based recommender Systems which will the focus of our next module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
