{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens - 100K Dataset\n",
    "\n",
    "MovieLens 100K dataset has been a standard dataset used for benchmarking recommender systems for more than 20 years now and hence this provides a good point to start our learning journey for recommender systems. For non commercial personalised recommendations for movies you can check out the website: https://movielens.org/\n",
    "\n",
    "This data set consists of:\n",
    "\t* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "\t* Each user has rated at least 20 movies. \n",
    "        * Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n",
    "The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. \n",
    "\n",
    "## Data Description\n",
    "\n",
    "\n",
    "**Ratings**    -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
    "              Each user has rated at least 20 movies.  Users and items are\n",
    "              numbered consecutively from 1.  The data is randomly\n",
    "              ordered. This is a comma separated list of \n",
    "\t         user id | item id | rating | timestamp. \n",
    "              The time stamps are unix seconds since 1/1/1970 UTC   \n",
    "\n",
    "\n",
    "**Movie Information**   -- Information about the items (movies); this is a comma separated\n",
    "              list of\n",
    "              movie id | movie title | release date | unknown | Action | Adventure | Animation |\n",
    "              Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "              Thriller | War | Western |\n",
    "              The last 19 fields are the genres, a 1 indicates the movie\n",
    "              is of that genre, a 0 indicates it is not; movies can be in\n",
    "              several genres at once.\n",
    "\n",
    "\n",
    "**User Demographics**    -- Demographic information about the users; this is a comma\n",
    "              separated list of\n",
    "              user id | age | gender | occupation | zip code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "[1. Reading Dataset](#Reading-Dataset)\n",
    "\n",
    "[2. Merging Movie information to ratings dataframe](#merge)\n",
    "\n",
    "[3. Creating train and test data & setting evaluation metric](#eval)\n",
    "\n",
    "[7. Importing Surprise & Loading Dataset](#dataload)\n",
    "\n",
    "[8. Fitting SVD Model with 100 latent factors on train set and checking performance on test set](#svdfit)\n",
    "\n",
    "[9. Examining Item and User matrices](#examine)\n",
    "\n",
    "[10. Grid Search for better performance with SVD](#gridsearch)\n",
    "\n",
    "[11. What's next?](#whatsnext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Dataset <a class=\"anchor\" id=\"Reading-Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading ratings file:\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "\n",
    "#Reading Movie Info File\n",
    "movie_info = pd.read_csv('movie_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Merging Movie information to ratings dataframe <a class=\"anchor\" id=\"merge\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie names are contained in a separate file. Let's merge that data with ratings and store it in ratings dataframe. The idea is to bring movie title information in ratings dataframe as it would be useful later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.merge(movie_info[['movie id','movie title']], how='left', left_on = 'movie_id', right_on = 'movie id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "      <th>movie id</th>\n",
       "      <th>movie title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "      <td>242</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "      <td>302</td>\n",
       "      <td>L.A. Confidential (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "      <td>377</td>\n",
       "      <td>Heavyweights (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "      <td>51</td>\n",
       "      <td>Legends of the Fall (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "      <td>346</td>\n",
       "      <td>Jackie Brown (1997)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  unix_timestamp  movie id  \\\n",
       "0      196       242       3       881250949       242   \n",
       "1      186       302       3       891717742       302   \n",
       "2       22       377       1       878887116       377   \n",
       "3      244        51       2       880606923        51   \n",
       "4      166       346       1       886397596       346   \n",
       "\n",
       "                  movie title  \n",
       "0                Kolya (1996)  \n",
       "1    L.A. Confidential (1997)  \n",
       "2         Heavyweights (1994)  \n",
       "3  Legends of the Fall (1994)  \n",
       "4         Jackie Brown (1997)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also combine movie id and movie title separated by ': ' and store it in a new column named movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['movie'] = ratings['movie_id'].map(str) + str(': ') + ratings['movie title'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'movie_id', 'rating', 'unix_timestamp', 'movie id',\n",
       "       'movie title', 'movie'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the columns movie, user_id and rating in the ratings dataframe and drop all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop(['movie id', 'movie title', 'movie_id','unix_timestamp'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[['user_id','movie','rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Train & Test Data & Setting Evaluation Metric <a class=\"anchor\" id=\"eval\"></a>\n",
    "In order to test how well we do with a given rating prediction method, we would first need to define our train and test set, we will only use the train set to build different models and evaluate our model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X as the original ratings dataframe\n",
    "X = ratings.copy()\n",
    "\n",
    "#Split into training and test datasets\n",
    "X_train, X_test = train_test_split(X, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that computes the root mean squared error (or RMSE)\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Importing Surprise & Loading Dataset <a class=\"anchor\" id=\"dataload\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing functions to be used in this notebook from Surprise Package\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a dataset from a pandas dataframe within Surprise, you will need the load_from_df() method. \n",
    "1. You will also need a `Reader` object and the `rating_scale` parameter must be specified. \n",
    "2. The dataframe here must have three columns, corresponding to the user (raw) ids, the item (raw) ids, and the ratings in this order. \n",
    "3. Each row thus corresponds to a given rating. This is not restrictive as you can reorder the columns of your dataframe easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reader object to import ratings from X_train\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "#Storing Data in surprise format from X_train\n",
    "data = Dataset.load_from_df(X_train[['user_id','movie','rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fitting SVD Model with 100 latent factors on train set and checking performance on test set <a class=\"anchor\" id=\"svdfit\"></a>\n",
    "\n",
    "Here we first fit an arbitrary model with 100 factors and check performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f8b4b748250>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new SVD with 100 latent features (number was chosen arbitrarily)\n",
    "model = SVD(n_factors=100)\n",
    "\n",
    "#Build full trainset will essentially fits the knnwithmeans on the complete train set instead of a part of it\n",
    "#like we do in cross validation\n",
    "model.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408283897758423"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id pairs for test set\n",
    "id_pairs = zip(X_test['user_id'], X_test['movie'])\n",
    "\n",
    "#Making predictions for test set using predict method from Surprise\n",
    "y_pred = [model.predict(uid = user, iid = movie)[3] for (user, movie) in id_pairs]\n",
    "\n",
    "#Actual rating values for test set\n",
    "y_true = X_test['rating']\n",
    "\n",
    "# Checking performance on test set\n",
    "rmse(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see merit in using SVD algorithm as it performs even better than the user based or item based techniques discussed earlier. Surprise provides ways to extract and read the user matrix and the item matrix separately as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examining the user and item matrices <a class=\"anchor\" id=\"examine\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise SVD stores the item matrix under the `model.qi` attribute and user matrix in `model.pu` attribute. First let us check the number of unique movies and users in the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1642, 943)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of movies & users in train data\n",
    "X_train.movie.nunique(), X_train.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 1642 movies with 943 users in the train set we created. Now let us check the shape of use and item matrices separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1642, 100), (943, 100), 1642, 943)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1642*100 (movie matrix)  943*100 (user matrix) # 1642*943 (user movie matrix)\n",
    "model.qi.shape, model.pu.shape,X_train.movie.nunique(), X_train.user_id.nunique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also find the reduction in the dimensionality of our original rating matrix and calculate the percentage reduction from these shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.30541214642672"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Percentage reduction in size wrt user item matrix\n",
    "(1642*943 - 943*100 - 1642*100)/(1642*943)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out its massive 83% means there is significant reduction in memory usage as compared to the simple neighbourhood based techniques with similar performance. Now, lets check the factors for a given movie. \n",
    "\n",
    "Surprise assigns its own ids to the items and users and we need to first extract that in order to identify the indec for a given movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01455032,  0.10667637,  0.1730418 , -0.05183758,  0.15130637,\n",
       "       -0.16141819, -0.05289471,  0.12656016,  0.05473493, -0.00115853,\n",
       "       -0.29084211])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting id for Toy story within qi matrix\n",
    "movie_row_idx = model.trainset._raw2inner_id_items['1: Toy Story (1995)']\n",
    "np.array(model.qi[movie_row_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent factors learnt from Funk SVD\n",
    "ts_vector = np.array(model.qi[movie_row_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting id for Wizard of Oz within qi matrix\n",
    "movie_row_idx = model.trainset._raw2inner_id_items['132: Wizard of Oz, The (1939)']\n",
    "woz_vector = np.array(model.qi[movie_row_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the similarity in latent factors for wizard of oz & Toy Story\n",
    "from scipy import spatial\n",
    "1 - spatial.distance.cosine(ts_vector,woz_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! the latent factors of Wizard of Oz and Toy Story have a correlation of 1. As an exercise you can use these similarities as weights to do an item based collaborative filtering and see if there is any improvement. Here we will use Grid search now to find out the optimal value of number of factors and other hyperparameters and ultimately improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Grid Search for better performance with SVD <a class=\"anchor\" id=\"gridsearch\"></a>\n",
    "We will try to optimize for number of factors and check cross validation score with 5 folds. We also need to set the random state here as the initialization clearly depends on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9449890479677279\n",
      "{'n_factors': 11, 'n_epochs': 20, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "#Defining the parameter grid for SVD and fixing the random state\n",
    "param_grid = {'n_factors':list(range(1,50,5)), 'n_epochs': [5, 10, 20], 'random_state': [42]}\n",
    "\n",
    "#Defining the grid search with the parameter grid and SVD algorithm optimizing for RMSE\n",
    "gs = GridSearchCV(SVD, \n",
    "                  param_grid, \n",
    "                  measures=['rmse'], \n",
    "                  cv=5, \n",
    "                  n_jobs = -1)\n",
    "\n",
    "#Fitting the mo\n",
    "gs.fit(data)\n",
    " \n",
    "#Printing the best score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "#Printing the best set of parameters\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f8b338bdd10>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model on train data with the best parameters\n",
    "model = SVD(n_factors = 11, n_epochs = 20, random_state = 42)\n",
    "\n",
    "#Build full trainset will essentially fits the SVD on the complete train set instead of a part of it\n",
    "#like we do in cross validation for grid search\n",
    "model.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9390125163978545"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id pairs for test set\n",
    "id_pairs = zip(X_test['user_id'], X_test['movie'])\n",
    "\n",
    "#Making predictions for test set using predict method from Surprise\n",
    "y_pred = [model.predict(uid = user, iid = movie)[3] for (user, movie) in id_pairs]\n",
    "\n",
    "#Actual rating values for test set\n",
    "y_true = X_test['rating']\n",
    "\n",
    "# Checking performance on test set\n",
    "rmse(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What's Next? <a class=\"anchor\" id=\"whatsnext\"></a>\n",
    "There are a couple of more parameters that you may tune to improve the performance further by controling overfitting, these are:\n",
    "1. Learning Rate (lambda) (lr_all)\n",
    "2. Regularization Parameter (gamma)  (reg_all)\n",
    "\n",
    "There are alternates to SVD such as SVD++ and other matrix factorization methods as well. However, this is out of scope of the course, you may go through these techniques in detail and implement them by utilising the documentation for Surprise Library: https://surprise.readthedocs.io/en/stable/matrix_factorization.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
